\documentclass[
	a4paper,
	oneside,
	% BCOR = 10mm,
	DIV = 12,
	fontsize = 13pt,
	headings = normal,
	numbers = endperiod,
]{scrartcl}

%%% Length calculations
\usepackage{calc}
%%%

%%% Support for color
\usepackage{xcolor}
\definecolor{lightblue}{HTML}{03A9F4}
\definecolor{red}{HTML}{F44336}
%%%

%%% Including graphics
\usepackage{graphicx}
%%%

%%% Font selection
\usepackage{fontspec}

\setromanfont{STIX Two Text}[
	Path              = {./fonts/},
	UprightFont       = {STIX2Text-Regular.otf},
	ItalicFont        = {STIX2Text-Italic.otf},
	BoldFont          = {STIX2Text-Bold.otf},
	BoldItalicFont    = {STIX2Text-BoldItalic.otf},
	SmallCapsFeatures = {LetterSpace = 8},
]

\setsansfont{IBM Plex Sans}[
	Path              = {./fonts/},
	UprightFont       = {IBMPlexSans-Regular.otf},
	ItalicFont        = {IBMPlexSans-Italic.otf},
	BoldFont          = {IBMPlexSans-Bold.otf},
	BoldItalicFont    = {IBMPlexSans-BoldItalic.otf},
	Scale = MatchUppercase,
]

\setmonofont{IBM Plex Mono}[
	Path              = {./fonts/},
	UprightFont       = {IBMPlexMono-Regular.otf},
	ItalicFont        = {IBMPlexMono-Italic.otf},
	BoldFont          = {IBMPlexMono-Bold.otf},
	BoldItalicFont    = {IBMPlexMono-BoldItalic.otf},
	Scale = MatchUppercase,
]
%%%

%%% Math typesetting
\usepackage{amsmath}

\usepackage{amsthm}
\newtheoremstyle{mythm}%
	{1\baselineskip} % Space above
	{1\baselineskip} % Space below
	{\upshape} % Body font
	{} % Indent amount
	{\upshape\bfseries} % Theorem head font
	{.} % Punctuation after theorem head
	{0.5em} % Space after theorem head
	{} % Theorem head spec
\theoremstyle{mythm}
\newtheorem{mythm}{Теорема}
\newtheorem{mydef}{Визначення}

\usepackage{IEEEtrantools}

\usepackage{unicode-math}
\setmathfont{STIX Two Math}
%%%

%%% List settings
\usepackage{enumitem}
\setlist[enumerate]{
	label*      = {\arabic*.},
	leftmargin  = *,
	labelindent = \parindent,
	topsep      = 1\baselineskip,
	parsep      = 0\baselineskip,
	itemsep     = 1\baselineskip,
	noitemsep,
}

\setlist[itemize]{
	label*      = {—},
	leftmargin  = *,
	% labelindent = \parindent,
	align       = left,
	topsep      = 1\baselineskip,
	parsep      = 0\baselineskip,
	itemsep     = 0\baselineskip,
}

\setlist[description]{
	font        = {\rmfamily\upshape\bfseries},
	topsep      = 1\baselineskip,
	parsep      = 0\baselineskip,
	itemsep     = 0\baselineskip,
}

\newlist{termpaperinfo}{enumerate}{3}
\setlist[termpaperinfo]{
	label*      = {\arabic*.},
	leftmargin  = *,
	align       = left,
	topsep      = 1\baselineskip,
	parsep      = 0\baselineskip,
	itemsep     = 1\baselineskip,
}

%%%

%%% Structural elements typesetting
\setkomafont{pagenumber}{\rmfamily\upshape}
\setkomafont{disposition}{\rmfamily\bfseries}

% Sectioning
\RedeclareSectionCommand[
	beforeskip = -1\baselineskip,
	afterskip  = 1\baselineskip,
	font       = {\normalsize\bfseries},
]{section}

% \renewcommand*{\sectionformat}{\thesection\autodot\enskip}

\renewcommand{\sectionlinesformat}[4]{%
	\centering{}#3#4%
}

\RedeclareSectionCommand[
	beforeskip = -1\baselineskip,
	afterskip  = 1\baselineskip,
	font       = {\normalsize\bfseries},
]{subsection}

\RedeclareSectionCommand[
	beforeskip = -1\baselineskip,
	afterskip  = 1\baselineskip,
	font       = {\normalsize\bfseries},
]{subsubsection}

\RedeclareSectionCommand[
	beforeskip = -1\baselineskip,
	afterskip  = -0.5em,
	font       = {\normalsize\mdseries\scshape\addfontfeatures{Letters = {UppercaseSmallCaps}}},
]{paragraph}
%%%

%%% Typographic enhancements
\usepackage{microtype}
%%%

%%% Language-specific settings
\usepackage{polyglossia}
\setmainlanguage{ukrainian}
\setotherlanguages{english}
%%%

%%% Captions
\usepackage{caption}
\usepackage{subcaption}

%\DeclareCaptionLabelFormat{closing}{#2)}
%\captionsetup[subtable]{labelformat = closing}

%\captionsetup[subfigure]{labelformat = closing}

\captionsetup[table]{
	aboveskip = 0\baselineskip,
	belowskip = 0\baselineskip,
}

\captionsetup[figure]{
	aboveskip = 1\baselineskip,
	belowskip = 0\baselineskip,
}

\captionsetup[subfigure]{
	labelformat = simple,
	labelformat = brace,
}
%%%

%%% Change counters
\usepackage{chngcntr}
\counterwithin{equation}{section}
%%%

%%% Hyphenated ragged typesetting
\usepackage{ragged2e}
%%%

%%% Table typesetting
\usepackage{booktabs}
\usepackage{longtable}

\usepackage{multirow}

\usepackage{array}
\newcolumntype{v}[1]{>{\RaggedRight\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{b}[1]{>{\Centering\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{n}[1]{>{\RaggedLeft\arraybackslash\hspace{0pt}}p{#1}}
%%%

%%% Drawing
\usepackage{tikz}
\usepackage{tikzscale}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows.meta} % Stealth arrow tips
\usetikzlibrary{shapes.multipart}
\usetikzlibrary{decorations.pathreplacing}
%%%

%%% SI units typesetting
\usepackage{siunitx}
\sisetup{
	output-decimal-marker = {,},
	exponent-product      = {\cdot},
	inter-unit-product    = \ensuremath{{} \cdot {}},
	per-mode              = symbol,
}
%%%

%%% Code highlighting
\usepackage{minted}

\newmintinline[pyinline]{python}{
	style = bw,
	breaklines,
	% breakbytokenanywhere,
	breakafter = { _},
	breakaftersymbolpre = {\textrm{-}},
}

\newminted{bash}{
	linenos,
	style = bw,
	breaklines,
	breakbytokenanywhere,
	autogobble, % automatically determine how many characters to gobble
}
%%%

%%% Framing code listings
\usepackage{tcolorbox}
\tcbuselibrary{breakable}
\tcbuselibrary{minted}
\tcbuselibrary{skins}

\newtcblisting[auto counter, list inside, number within = section]{listingpython}[3][]{%
	minted language = python,
	minted style    = bw,
	minted options  = {
		linenos,
		tabsize = 4,
		breaklines,
		% breakanywhere,
		fontsize = \footnotesize,
		autogobble
	},
	%
	% empty,
	sharp corners,
	colframe         = black,
	colback          = black!0,
	leftrule         = 0em,
	rightrule        = 0em,
	toprule          = 0pt, % orig = 0pt
	bottomrule       = 0pt, % orig = 0pt
	titlerule        = 0.5pt,
	colbacktitle     = black!0,
	coltitle         = black,
	toptitle         = 0.3em,
	bottomtitle      = 0.1em,
	borderline north = {1pt}{0pt}{black},
	borderline south = {1pt}{0pt}{black},
	before skip      = \intextsep,
	after  skip      = \intextsep,
	title            = {Лістинг \thetcbcounter: #2},
	list entry       = {\protect\numberline{\thetcbcounter}#2},
	left = 0em,
	right = 0em,
	%
	listing only,
	breakable,
	%
	label = {#3},
	%
	#1
}

\newtcbinputlisting[auto counter, list inside, number within = section]{\inputpython}[4][]{%
	minted language = python,
	minted style    = bw,
	minted options  = {
		linenos,
		tabsize = 4,
		breaklines,
		breakbytokenanywhere,
		fontsize = \footnotesize,
	},
	%
	% empty,
	sharp corners,
	colframe         = black,
	colback          = black!0,
	leftrule         = 0em,
	rightrule        = 0em,
	toprule          = 0pt, % orig = 0pt
	bottomrule       = 0pt, % orig = 0pt
	titlerule        = 0.5pt,
	colbacktitle     = black!0,
	coltitle         = black,
	toptitle         = 0.3em,
	bottomtitle      = 0.1em,
	borderline north = {1pt}{0pt}{black},
	borderline south = {1pt}{0pt}{black},
	before skip      = \intextsep,
	after  skip      = \intextsep,
	title            = {Лістинг \thetcbcounter: #3},
	list entry       = {\protect\numberline{\thetcbcounter}#3},
	left = 0em,
	right = 0em,
	%
	listing file={#2},
	listing only,
	breakable,
	%
	label = {#4},
	%
	#1
}

% Customize minted line numbers
\renewcommand{\theFancyVerbLine}{\ttfamily\scriptsize\arabic{FancyVerbLine}}

%%%

%%% Bibliography
\usepackage[
	style    = gost-numeric,
	language = auto,
	autolang = other,
	sorting  = none,
]{biblatex}
\addbibresource{y03s01-syssoft-term-paper-01-bibliography.bib}
%%%

%%% Links and hyperreferences
\usepackage{hyperref}
\hypersetup{
	bookmarksnumbered = true,
	colorlinks      = false,
	linkbordercolor = red,
	urlbordercolor  = lightblue,
	pdfborderstyle  = {/S/U/W 1.5},
}
%%%

%%% Length adjustments
% Set baselineskip, default is 14.5 pt
\linespread{1.068966} % ~15.5 pt
\setlength{\emergencystretch}{1em}
\setlength{\parindent}{1.5em}
\newlength{\gridunitwidth}
\setlength{\gridunitwidth}{\textwidth / 12}
%%%

%%% Custom commands
\newcommand{\blankspace}[1]{\underline{\hspace{#1}}}
\newcommand{\allcaps}[1]{{\addfontfeatures{LetterSpace = 8, Kerning = Off}#1}}
\newcommand{\filename}[1]{\texttt{#1}}
\newcommand{\progname}[1]{\texttt{#1}}
\newcommand{\modulename}[1]{\texttt{#1}}

\newcommand{\myvec}[1]{\mathbf{#1}}
%%%

%%% Custom math commands
\DeclareMathOperator{\classify}{classify}
\DeclareMathOperator*{\argmax}{arg\,max} % starred to place subsctipt below function name in display style
\DeclareMathOperator*{\argmin}{arg\,min} % starred to place subsctipt below function name in display style
%%%

\begin{document}

\begin{titlepage}
		\begin{center}
			Міністерство освіти і науки України\\
			Національний авіаційний університет\\
			Навчально-науковий інститут комп'ютерних інформаційних технологій\\
			Кафедра комп'ютеризованих систем управління

			\vspace{\fill}
				Курсова робота\\
				з дисципліни «Системне програмне забезпечення»\\

				\vspace*{3\baselineskip}

				Пояснювальна записка\\
				Тема: реалізація наївного баєсового класифікатора на~мові~програмування~\textenglish{Python}

			\vspace{\fill}

			\begin{flushright}
				Виконав:\\
				студент групи СП-325\\
				Клокун В.\,Д.\\
			\end{flushright}

			Київ — 2018
		\end{center}
	\end{titlepage}

	\section*{Завдання на~виконання курсової~роботи\\студента групи~СП-325 Клокуна Владислава~Денисовича}
	% {\centering{}студента групи~СП-325 Клокуна Владислава Денисовича\par}
	\begin{termpaperinfo}
		\item Тема курсової роботи: реалізація наївного баєсового класифікатора на~мові~програмування~\textenglish{Python} для класифікації спостережень, що~містять неперервні дані.
		\item Термін виконання курсової роботи:\\ з~«\blankspace{1cm}» \blankspace{4cm}~2018~р. по~«\blankspace{1cm}»~\blankspace{4cm}~2018~р.
		\item Вхідні дані до роботи: набір даних для класифікації.
		\item Етапи виконання курсової роботи:
			\begin{itemize}
				\item Огляд теоретичних відомостей про наївний баєсов класифікатор.
				\item Реалізація та тестування наївного баєсового класифікатора.
			\end{itemize}
		\item Перелік обов'язкових додатків і графічного матеріалу:
			\begin{itemize}
				\item FIXME.
			\end{itemize}
	\end{termpaperinfo}

	{%
		\newlength{\blanklinematch}
		\setlength{\blanklinematch}{1cm + \widthof{» } + 5cm}
		\noindent%
		\begin{tabular}{
				@{}ll
			}
			Завдання отримав: & «\blankspace{1cm}» \blankspace{5cm}~2018~р.\\
			Підпис студента:  & \phantom{«}\blankspace{\blanklinematch}~(Клокун В.\,Д.)\\
			% Підпис студента:  & \blankspace{\widthof{«} + 1cm + \widthof{»} + 5cm}~(Клокун В.\,Д.)
		\end{tabular}
		\par
	}

	\newpage
	\tableofcontents

	\newpage
	\section{Теоретична частина}
		\subsection{Короткі теоретичні відомості}
			\label{ssec:theory-short}
			Припустимо, що~в~ході деякого експерименту проводились спостереження, під час проведення яких збирались неперервні (недискретні) дані про~результат події. Також були визначені категорії (або класи), до~яких ці~дані можуть належати. Поставлена задача класифікувати дані спостережень. \emph{Класифікація}~— це~задача визначення, до~якої з~категорій належить певне спостереження~\cite{wiki-stat-classification}. \emph{Класифікатор}~— це~алгоритм, який виконує класифікацію~\cite{wiki-stat-classification}.

			\emph{Наївний баєсів класифікатор}~— це ймовірнісний класифікатор, який використовує теорему Баєса для класифікації спостережень. Такі класифікатори отримують на вхід спостереження, оцінюють його і~роблять припущення про~клас, до~якого воно~належить. Вхідні дані, тобто спостереження, представляються у~вигляді вектора відомих значень випадкових змінних, які називаються \emph{ознаками}. Результатом роботи класифікатора є~певне значення цільової змінної або змінних, які зазвичай називаються класовими, і позначають клас, до~якого належить спостереження.

			Принцип класифікації полягає в~обчисленні умовних імовірностей~(визначення~\ref{def:conditional-probability}) того, що~вхідні дані належать до~певних класів~(події, які~нас цікавлять), за~умови, що~ознаки мають певні значення~(події, які ми спостерігаємо). Після обчислення кожної з~умовних імовірностей за~обраним правилом прийняття рішення~робиться висновок, до якого класу належить задане спостереження. Оскільки такий класифікатор використовує ймовірнісну модель, наївний баєсів класифікатор називають~\emph{імовірнісним}.

			\begin{mydef}[Умовна ймовірність]
				\label{def:conditional-probability}
				Нехай~$A$ і~$B$~— події. Позначимо ймовірність настання кожної з~них незалежно одна від~одної як~$P(A)$ і~$P(B)$ відповідно. Тоді \emph{умовною імовірністю}~$P(A \mid B)$ називається ймовірність настання події~$A$ за~умови, що~подія~$B$ настала. Вона обчислюється так:
				\begin{IEEEeqnarray}{rCl}
					P(A \mid B) = \frac{P(A \cap B)}{P(B)},
				\end{IEEEeqnarray}
				де~$P(A \cap B)$~— ймовірність, що події~$A$ і~$B$ настали. 
			\end{mydef}
			
			Розглянемо приклад класифікації наївним баєсовим класифікатором. Нехай подія~$A$~— дане спостереження належить до~певного класу, подія~$B$~— ознаки спостереження мають певні значення. Тоді щоб знайти ймовірність, що дане спостереження з~певним значенням ознак належить до~певного класу, необхідно обчислити умовну ймовірність~$P(A \mid B)$. Для~обчислення цієї імовірності необхідно використати теорему~Баєса~(теорема~\ref{thm:theorem-bayes}).

			\begin{mythm}[Баєса]
				\label{thm:theorem-bayes}
				Нехай~$P(A \mid B)$~— умовна ймовірність настання події~$A$ за~умови, що~подія~$B$ настала, $P(B \mid A)$~— умовна ймовірність настання події~$B$ за~умови, що~подія~$A$ настала; $P(B)$~— імовірність настання події~$B$, причому~$P(B) \neq 0$. Тоді умовна ймовірність~$P(A \mid B)$ обчислюється так:
				\begin{IEEEeqnarray}{rCl}
					P(A \mid B) = \frac{P(B \mid A) \, P(A)}{P(B)}.
				\end{IEEEeqnarray}
			\end{mythm}

			\begin{proof}
				Виразимо ймовірності~$P(A \mid B)$ і~$P(B \mid A)$ за~визначенням умовної ймовірності:
				\begin{IEEEeqnarray}{rCl}
					\label{eq:p-ab}
					P(A \mid B) &=& \frac{P(A \cap B)}{P(B)},\, P(B) \neq 0,\\ \IEEEstrut[4\jot]
					\label{eq:p-ba}
					P(B \mid A) &=& \frac{P(B \cap A)}{P(A)},\, P(A) \neq 0.
				\end{IEEEeqnarray}
				Представимо ймовірності настання подій~$A$ і~$B$ разом, тобто~$P(A \cap B)$ і~$P(B \cap A)$ з~рівностей~\ref{eq:p-ab}, \ref{eq:p-ba}:
				\begin{IEEEeqnarray}{rCl}
					P(A \cap B) &=& P(A \mid B) \, P(B),\\
					P(B \cap A) &=& P(B \mid A) \, P(A).
				\end{IEEEeqnarray}
				Оскільки ліві частини рівні за аксіомою~$P(A \cap B) = P(B \cap A)$, то і~праві частини також будуть рівними: $P(A \mid B) \, P(B) = P(B \mid A) \, P(A)$, отже можна записати, що:
				\begin{IEEEeqnarray}{rCl+x*}
					\label{eq:pab-eq-pba-pb}
					P(A \cap B) = P(B \mid A) \, P(A).
				\end{IEEEeqnarray}
				Підставимо рівність~\ref{eq:pab-eq-pba-pb} в~\ref{eq:p-ab} і~отримаємо:
				\begin{IEEEeqnarray}{rCl+x*}
					P(A \mid B) &=& \frac{P(A \cap B)}{P(B)} = \frac{P(B \mid A) \, P(A)}{P(B)},\, P(B) \neq 0.
					\\* &&& \qedhere\nonumber
				\end{IEEEeqnarray}
			\end{proof}

			Як видно, при~класифікації наївним баєсовим класифікатором, у~обчисленні умовних імовірностей використовується теорема Баєса, тому~такий імовірнісний класифікатор називається \emph{баєсовим}.

		\subsection{Імовірнісна модель наївного баєсового класифікатора}
			Як було сказано у~підрозділі~\ref{ssec:theory-short}, наївний баєсовий класифікатор використовує ймовірнісну модель. Побудуємо та~представимо~її. Для~виконання класифікації необхідні вхідні дані, тобто спостереження, та~набір можливих значень класової змінної для~позначення класів, до~яких можуть належати ці~дані. Позначатимемо змінні, на~кшталт~$X_i$, великими літерами, а~їх~значення, наприклад, $x_i$~— малими. Вектори, на~зразок~$\myvec{X}$~— жирним шрифтом.
			
			Вхідними даними для~класифікації буде вектор ознак~$\myvec{X} = \left( X_1, \dots, X_n \right)$, де~$X_1, \dots, X_n$~— ознаки. Кожна ознака може мати значення зі~своєї області визначення, яка позначається~$D_i$. Набір усіх векторів ознак позначається як~$\Omega = D_1 \times \dots \times D_n$. Для позначення класу, до якого належить спостереження, введемо випадкову змінну~$C$, де~$C$ може приймати одне з~$m$ значень: $c \in \{ 0, \dots, m - 1\}$. 

			\begin{mydef}[Розподіл імовірностей]
				Нехай~$X$ і~$Y$~— випадкові змінні, які приймають значення~$x$ та~$y$ відповідно. Тоді розподіл імовірностей~$p(X \mid Y)$ позначає значення імовірностей~$P(X = x_i \mid Y = y_i)$ для кожної з можливих пар~$i, j$.~\cite{russel-norvig-ai-ma}
			\end{mydef}

			Класифікація за допомогою наївного баєсового класифікатора ставить у~відповідність кожному вектору~$\myvec{X}$, який~містить ознаки~$X_1, \dots, X_n$, розподіли ймовірностей~$p(C \mid \myvec{X})$. Тобто сама модель має такий загальний вигляд:
			\begin{equation}
				p\left( C \mid \myvec{X} \right) = p\left( C \mid X_1, \dots, X_n \right).
			\end{equation}
			Зі~зростанням кількості або~можливих значень ознак, з~такою моделлю неможливо працювати за~допомогою таблиць імовірностей, тому~переформулюємо модель, щоб~зробити її зручнішою.

			Використовуючи теорему Баєса, представимо її так:
			\begin{equation}
				p(C \mid X_1, \dots, X_n) = \frac{p(C) \, p(X_1, \dots, X_n \mid C)}{p( X_1, \dots, X_n)}.
			\end{equation}
			Видно, що~дільник не~залежить від~змінної~$C$, а~значення ознак~$X_i$ задані наперед, тому на~практиці значення дільника постійне. Ділене рівносильне такій моделі спільного розподілу:
			\begin{equation}
				p(C, X_1, \dots, X_n).
			\end{equation}
			Перетворюємо дану модель за~допомогою визначення умовної ймовірності:
			\begin{IEEEeqnarray}{rCl}
				p(C, X_1, \dots, X_n) &=& p(C) \, p(X_1, \dots, X_n \mid C)\\
				                      &=& p(C) \, p(X_1 \mid C) \, p(X_2, \dots, X_n \mid C, X_1)\\
															&=& p(C) \, p(X_1 \mid C) \, p(X_2 \mid C, X_1) \, p(X_3, \dots, X_n \mid C, X_1, X_2)\: \\
															&=& p(C) \, p(X_1 \mid C) \, p(X_2 \mid C, X_1) \, p(X_3 \mid C, X_1, X_2) \nonumber\\
															&& \> p(X_4, \dots, X_n \mid C, X_1, X_2, X_3)
			\end{IEEEeqnarray}
			і~так далі. Тепер припускаємо, що кожна ознака~$X_i$ умовно незалежна від~кожної іншої ознаки~$X_j$, для будь-яких~$j \neq i$ та~заданої категорії~$C = c$. Математично це означає:
			\begin{IEEEeqnarray*}{rCl}
				p (X_i \mid C, X_j) = p (X_i \mid C).
			\end{IEEEeqnarray*}
			Таке припущення є наївним, оскільки немає жодних підстав вважати, що вхідні ознаки дійсно незалежні одна від одної. Саме тому такий баєсовий класифікатор називається \emph{наївним}.

			% \begin{IEEEeqnarray*}{rCl}
			% 	p(C, X_1, \dots, X_n) &=& p( X_1, \dots, X_n, C) \\
			% 												&=& p( X_1 \mid X_2, \dots, X_n, C) \, p(X_2, \dots, X_n, C) \\
			% 												&=& p( X_1 \mid X_2, \dots, X_n, C) \, p( X_2 \mid X_3, \dots, X_n, C) \, p(X_3, \dots, X_n, C) \\
			% 												&\dots& \\
			% 												&=& p( X_1 \mid X_2, \dots, X_n, C) \, p( X_2 \mid X_3, \dots, X_n, C) \dots p(X_{n-1} \mid X_n, C) \, p(X_n | C) \, p(C). \\
			% \end{IEEEeqnarray*}

			Отже, виражаємо загальну модель:
			\begin{IEEEeqnarray}{rCl}
				p(C \mid X_1, \dots, X_n) &=& p(C) \, p(X_1 \mid C) \, p(X_2 \mid C) \dots p(X_n \mid C) \\
																	&=& p(C) \prod_{i=1}^{n} p(X_i \mid C).
			\end{IEEEeqnarray}
			Це означає, що враховуючи припущення про незалежність змінних, умовний розподіл над класовою змінною~$C$ може бути виражений так:
			\begin{IEEEeqnarray}{rCl}
				p(C \mid X_1, \dots, X_n) = \frac{1}{Z} \, p(C) \prod_{i=1}^{n} p( X_i \mid C),
			\end{IEEEeqnarray}
			де~$Z$~— коефіцієнт,~який залежить виключно від~$X_1, \dots, X_n$. Якщо~значення ознак~$x_1, \dots, x_n$ відомі, коефіцієнт~$Z$ сталий.

			Таку модель значно зручніше використовувати, оскільки вони використовують апріорні імовірності класів~$p(C)$ та незалежні розподіли~$p(X_i \mid C)$. Якщо є~$k$~класів та модель модель для~$p(X_i)$ може бути виражена $r$~ параметрами, то відповідний наївний баєсовий класифікатор матиме~$(k - 1) + nrk$ параметрів.~\cite{murty-devy-pattern-rec}

		\subsection{Оцінка параметрів}
			Описавши ймовірнісну модель, необхідно визначити її параметри, тобто апріорні ймовірності належності до~класу~$p(C)$ та~розподіли ймовірностей ознак~$p(X_i \mid C)$. Для обчислення параметрів моделі використовують \emph{тренувальний набір даних}~— такий набір даних, який складається із заздалегідь класифікованих спостережень. Тобто набір даних~$S$ складатиметься з~векторів~$\myvec{x} = (x_1, \dots, x_n, c)$, де~$c$~— правильне значення класової змінної.
			
			Усі параметри моделі можна обчислити з~тренувального набору даних.~\cite{murty-devy-pattern-rec} Щоб оцінити значення параметрів, необхідно зробити припущення щодо розподілу, який характеризує дані. Припущення щодо розподілу, який характеризує дані, називають \emph{моделлю подій}. При роботі з~неперервними (недискретними) даними, зазвичай припускають, що~вони розподілені за~законом нормального (гаусового) розподілу. Функція густини ймовірності нормального розподілу така: 
			\begin{IEEEeqnarray}{rCl}
				f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x - \mu)^{2}}{2 \sigma^2}},
			\end{IEEEeqnarray}
			де $\mu$~— математичне сподівання, $\sigma^2$~— дисперсія випадкової величини.

			Наприклад, припустимо, що тренувальний набір даних містить неперервну ознаку~$X$. Щоб обчислити розподіл імовірності, необхідно спочатку розподілити дані за класами, наданими у тренувальному наборі, та обчислити математичне сподівання $\mu_{c}$ і~дисперсію випадкової величини~$\sigma_{c}^{2}$ для кожного з~класів. Нехай $\mu_{c_1}$~— математичне сподівання значень, які належать до класу~$c_1$, а~$\sigma_{c_1}^{2}$~— їх дисперсія. У результаті певного спостереження отримали значення~$x$. Тоді розподіл імовірності для значення~$x$ для класу~$c$ обчислюється так:
			\begin{IEEEeqnarray}{rCl}
				p(X = x \mid C = c) = \frac{1}{\sigma_{c} \sqrt{2 \pi}} \exp \left(-\frac{(x - \mu_{c})^{2}}{2 \sigma_{c}^{2}} \right).
			\end{IEEEeqnarray}

			Оцінивши усі необхідні параметри для моделі, тобто апріорні розподіли імовірності належності до~класу~$p(C)$ та~розподіли ймовірностей ознак~$p(X_i \mid C)$, можна переходити до~класифікації.  

		\subsection{Побудова класифікатора з~імовірнісної моделі}
			Наївний баєсовий класифікатор поєднує баєсову імовірнісну модель з~правилом прийняття рішення. Одним з~поширених правил є~вибір найбільш ймовірної гіпотези. Такий підхід називається \emph{правилом прийняття рішення за~максимальною апостеріорною імовірністю}~(\textenglish{maximum a posterior (\allcaps{MAP}) decision rule}). Відповідний класифікатор є~функцією~$\classify(X_1, \dots, X_n)$, яка визначається так:
			\begin{IEEEeqnarray}{rCl}
				\classify (X_1, \dots, X_n) = \argmax_{c} \left( p(C =c) \prod_{i=1}^{n} p(X_i = x_i \mid C = c) \right),
			\end{IEEEeqnarray}
			де~$\argmax_{x} f(x)$~— функція, результатом якої є множина значень~$x$, при якому значення функції~$f(x)$ максимальне~(визначення~\ref{def:argmax})~\cite{murty-devy-pattern-rec}. 
			
			\begin{mydef}[Функція~$\argmax$]
				\label{def:argmax}
				Нехай дана довільна множина~$X$, повністю впорядкована множина~$Y$ та~функція~$f \colon X \mapsto Y$, тоді функція~$\argmax$ для певного значення~$x$ над певною підмножиною~$S$ визначається так:
				\begin{IEEEeqnarray}{rCl}
					\argmax_{x \in S \subseteq X} f(x) \coloneq \{x \mid x\in S \wedge \forall y \in S : f(y) \leqslant f(x)\}.
				\end{IEEEeqnarray}
			\end{mydef}

			Правило прийняття рішення за максимальною апостеріорною імовірністю правильно класифікує спостереження за умови, що ймовірність належності до~правильного класу більша за~ймовірності належності до інших класів, тому немає потреби у надточній оцінці цих імовірностей. 

			Таким чином ми отримали працюючу теоретичну модель для~реалізації наївного баєсового класифікатора для класифікації спостережень, які~містять неперервні~(недискретні) дані.

	\newpage
	\section{Практична частина}

		\subsection{Використані програмні засоби}
			Для реалізації наївного баєсового класифікатора була використана мова програмування~\textenglish{Python} версії~3.7.1. Оскільки~\textenglish{Python}~— інтерпретована мова програмування, для~коректної роботи розробленої реалізації необхідно встановити робочий інтерпретатор~\textenglish{Python 3}, який~можна завантажити на~офіційному сайті за~посиланням~\url{https://python.org/downloads}. Також розроблена реалізація використовує~засоби стандартної бібліотеки мови програмування~\textenglish{Python}~(табл.~\ref{tab:used-modules}).
			
		%	, зокрема модулі~\modulename{argparse}, \modulename{csv}, \modulename{math} і~\modulename{random}. Розглянемо кожен з них:
			%Модуль~\modulename{argparse} призначений для~обробки аргументів командного рядка; модуль~\modulename{csv}~— для зчитування файлів у форматі~\filename{.csv}, які містять дані, розділені комою; модуль~\modulename{math} необхідний для зручного обчислення математичних функцій, зокрема~$e^x$, $a^b$ та~$\sqrt{x}$; модуль~\modulename{random} використовувався для~роботи з~випадковими числами, а~саме перемішування набору даних у~випадковому порядку.

			\begin{table}[!htbp]
				\caption{Перелік використаних модулів стандартної бібліотеки мови програмування~\textenglish{Python}}
				\label{tab:used-modules}
				\begin{tabular}{
						v{3\gridunitwidth - 2\tabcolsep}
						v{9\gridunitwidth - 2\tabcolsep}
				}
					\toprule
						Модуль & Призначення\\
					\midrule
						\modulename{argparse} & Обробка аргументів командного рядка.\\
						\modulename{csv} & Зчитування файлів у~форматі~\filename{.csv}, які містять дані, розділені комою.\\
						\modulename{math} & Зручне обчислення математичних функцій, зокрема $e^x$, $a^b$ та~$\sqrt{x}$.\\
						\modulename{random} & Робота з~випадковими числами, а~саме перемішування набору даних у~випадковому порядку.\\
					\bottomrule
				\end{tabular}
			\end{table}

		\subsection{Опис програми та роботи з~нею}
			Реалізація класифікатора~(яку далі ми називатимемо програмою) виконана у~вигляді двох модулів на~мові~\textenglish{Python}: \filename{nbc.py} і~\filename{nbc\_main.py}. Модуль~\filename{nbc.py} містить реалізації всіх функцій, необхідних для роботи наївного баєсового класифікатора, а~модуль~\filename{nbc\_main.py} містить інструкції, які~використовують надані функції для~класифікації наборів даних. 
			
			Розроблена програма виконує класифікацію неперервних чисельних даних із~набору даних, який знаходиться у~вхідному файлі формату~\textenglish{\allcaps{CSV}}. Вона запускається за~допомогою командного рядка таким чином:
			\begin{bashcode}
				python nbc_main.py input.csv
			\end{bashcode}
			Після виконання вищезазначеної команди, програма зчитує набір даних, вказаний у~файлі~\filename{input.csv}, оброблює та~класифікує~їх, виводить результати класифікації та~її~обчислену точність. Якщо вхідний файл не~зазначений, програма попереджує про~це користувача, надає інформацію про~правильний формат використання та~завершує роботу.

		\subsection{Опис процесу роботи програми}

			Модуль~\filename{nbc\_main.py}, який використовується для запуску програми, містить інструкції для~класифікації конкретного набору даних наївним баєсовим класифікатором. Він завантажує необхідні для роботи програми ресурси в~область видимості~(лістинг~\ref{lst:nbc-main}) та~описує загальний процес роботи програми.

			\begin{listingpython}{Модуль~\filename{nbc\_main.py}: завантаження ресурсів, необхідних для~роботи класифікатора}{lst:nbc-main}
				#!/usr/bin/env python3

				import argparse
				from nbc import *
			\end{listingpython}

			Перш за~все, у~файлі модуля вказаний шлях до~сумісного інтерпретатора, а~саме \textenglish{Python~3}. Це~надає можливість запускати модуль у~\textenglish{Unix}-подібних операційних системах як~скрипт, не~вказуючи шлях до~потрібного інтерпретатора явно.
			
			Інструкція~\pyinline{import argparse} завантажує в~область видимості модуль для~обробки аргументів командного рядка. Інструкція~\pyinline{from nbc import *} завантажує всі функції, описані у~модулі~\filename{nbc\_main.py}, щоб~їх~можна було використовувати у~поточному модулі. 

			Тепер, коли обробник параметрів командного рядка та~всі функції класифікатора завантажені, відбувається перевірка способу запуску модуля~(лістинг~\ref{lst:nbc-main-py-execution-check}). 

			\begin{listingpython}{Модуль~\filename{nbc\_main.py}: перевірка способу запуску модуля}{lst:nbc-main-py-execution-check}
if __name__ == '__main__':
parser = argparse.ArgumentParser(description = 'An implementation of Gaussian Naive Bayes Classifier in Python using stdlib facilities. Reads a CSV file containing floats.')

# Parse CSV dataset file
parser.add_argument('input', help = 'Path to input file')

args = parser.parse_args()

main(args)
			\end{listingpython}

			Якщо модуль запущений як~скрипт, наприклад, з~командного рядка, як~необхідно для~роботи розробленої реалізації, інтерпретатор встановлює особливе значення змінної~\pyinline{__name__}. Тому перевіряється умова~\pyinline{__name__ == '__main__'}. Якщо вона виконується, ініціалізується обробник параметрів командного рядка~\pyinline{argparse.ArgumentParser} з~коротким описом програми. Далі додається аргумент~\pyinline{'input'} зі~стислим поясненням до нього: він містить шлях до вхідного файлу, тобто файлу з~набором даних, які~необхідно класифікувати.

			Коли у~обробнику описані бажані аргументи, створюється змінна~\pyinline{args}, функція~\pyinline{parser.parse_args()} оброблює параметри та~зберігає~їх значення у~створеній змінній. Тепер, коли параметри оброблені, вони передаються у~функцію~\pyinline{main()}~— починаються основні етапи роботи програми~(лістинг~\ref{lst:nbc-main-py-main}).

			\begin{listingpython}{Модуль~\filename{nbc\_main.py}: функція~\pyinline{main()}, яка описує загальний процес роботи програми}{lst:nbc-main-py-main}
def main(args):
	dataset = load_training(args.input)
	dataset_train, dataset_test = split_dataset(dataset, 1/3) # split dataset 1 / 3

	summaries = summarize_by_class(dataset_test)
	predictions = predict_dataset(summaries, dataset_test)
	acc = compute_accuracy(dataset_test, predictions)
	print('Accuracy: {}'.format(acc))
				\end{listingpython}
			
				Загалом, процес роботи програми можна умовно поділити на~такі етапи:
				\begin{enumerate}
					\item Підготовка вхідних даних.
					\item Класифікація.
					\item Виведення результату.
				\end{enumerate}

				Розглянемо кожен етап роботи програми по~порядку.

				\subsubsection{Підготовка вхідних даних}

					Функція~\pyinline{main()}~(лістинг~\ref{lst:nbc-main-py-main}), а~з~нею і~етап підготовки вхідних даних, починається з~інструкції~\pyinline{dataset = load_training(args.input)}, яка~призначена для~завантаження файлу з~набором даних у~змінну~\pyinline{dataset} за~допомогою функції~\pyinline{load_training()}~(лістинг~\ref{lst:nbc-py-load-training}). Розглянемо роботу цієї функції детальніше.

					\begin{listingpython}{Модуль~\filename{nbc.py}: функція~\pyinline{load_training()} для завантаження набору даних у пам'ять}{lst:nbc-py-load-training}
	def load_training(filename):
			with open(filename) as file:
					# ignore all lines which start with '#'
					reader = csv.reader(row for row in file if not row.startswith('#'))
					dataset = []
					for row in reader:
							# comprehend each line as a list of floats and append it to the dataset
							dataset.append([float(x) for x in row])

			return dataset
					\end{listingpython}

					Функція~\pyinline{load_training()} відкриває файл, назва якого передається в аргументі~\pyinline{filename}, ініціалізує~\textenglish{\allcaps{CSV}}-зчитувач~\pyinline{csv.reader} у~змінну~\pyinline{reader}. Зчитувач ініціалізується всіма рядками файлу, які~не~починаються з~символу~«\verb|#|». Далі рядки, записані у~зчитувач, перетворюються у~вектори, які~містять ознаки у~форматі з~плаваючою комою. Перетворені вектори записуються у~змінну~\pyinline{dataset}. Тепер функція повертає змінну~\pyinline{dataset} та~завершує роботу.

					Далі у~процесі виконання програми виконується інструкція~\pyinline{dataset_train, dataset_test = split_dataset(dataset, 1/3)}. Вона ділить завантажений набір даних~\pyinline{dataset} на~тренувальний набір даних~\pyinline{dataset_train} і~тестовий набір даних~\pyinline{dataset_test} за допомогою функції~\pyinline{split_dataset}~(лістинг~\ref{lst:nbc-py-split-dataset}). Розподіл відбувається~таким чином, щоб~тренувальний набір даних \pyinline{dataset_train} містив як~мінімум \pyinline{ratio}~частину загального набору даних. Розбиття загального набору даних необхідне для~тестування точності роботи класифікатора і~є~загальноприйнятою практикою в~аналізі даних. 

				\begin{listingpython}{Модуль~\filename{nbc.py}: функція~\pyinline{split_dataset()} для~розбиття загального набору даних на~тренувальний та~тестовий набори}{lst:nbc-py-split-dataset}
	def split_dataset(dataset, ratio):
			dataset_shuffled = dataset
			random.shuffle(dataset_shuffled)
			dataset_train = dataset_shuffled[:int(len(dataset_shuffled) * ratio)]
			dataset_test = dataset_shuffled[int(len(dataset_shuffled) * ratio):]

			return dataset_train, dataset_test
					\end{listingpython}

					Функція~\pyinline{split_dataset()} копіює отриманий набір даних~\pyinline{dataset} у~змінну~\pyinline{dataset_shuffled}. Далі функція~\pyinline{random.shuffle()} перемішує вміст змінної~\pyinline{dataset_shuffled}, тобто початковий набір даних, випадковим чином, щоб усунути будь-які статистичні похибки (упередження) у~даних. Перемішування відбувається безпосередньо у~змінній, на~місці.
					
					Тепер дані діляться на~необхідні частини: зі~списку~\pyinline{dataset_shuffled} копіюється частина від~початку до~елементу з~індексом~\pyinline{int(len(dataset_shuffled) * ratio)} і~присвоюється змінній \pyinline{dataset_train}. Далі з~цього~ж списку копіюється частина від~елементу з~індексом~\pyinline{int(len(dataset_shuffled) * ratio)} до~кінця і~присвоюється змінній~\pyinline{dataset_test}~(рис.~\ref{fig:dataset-shuffled-slicing}).

					\begin{figure}[!htbp]
						\centering
						\begin{tikzpicture}[
								arraynode/.style = {
									draw,
									anchor = west,
									minimum height = {1.5\baselineskip},
									minimum width = {3\baselineskip},
									font = {\footnotesize},
								},
								arrayindex/.style = {
									anchor = west,
									minimum height = {1.5\baselineskip},
									minimum width = {1.5\baselineskip},
									font = {\footnotesize},
								},
						]

							% Legend
							\begin{scope}[
									every node/.style = {
										% draw,
										text width = 7\baselineskip,
										align = flush left,
										font = {\footnotesize},
										minimum width = 7\baselineskip,
										minimum height = 1.5\baselineskip,
									}
							]
								\node [] (c-index) at (0,0) {Індекси};
								\node [below = 1\baselineskip of c-index] (c-list) {Список \pyinline{dataset_shuffled}};
								% \node [below = of c-list] (c-res-list) {Результуючі списки};
							\end{scope}

							% Nodes
							\node [arraynode, right = of c-list] (n1) {Початок};
							\node [arraynode] (n2) at (n1.east) {\dots};
							% \node [arraynode] (n3) at (n2.east) {\phantom{\pyinline{int(len(dataset_shuffled) * ratio)}}};
							\node [arraynode] (n3) at (n2.east) {};
							\node [arraynode] (n4) at (n3.east) {\dots};
							\node [arraynode] (n5) at (n4.east) {Кінець};

							% Indices
							\node [arrayindex, right = of c-index] (i1) {0};
							\node [arrayindex] (i2) at (i1.east) {\pyinline{int(len(dataset_shuffled) * ratio)}};

							% Braces
							\begin{scope}[decoration = {brace, amplitude = {0.5em}}]
								\draw[decorate, decoration = {mirror}] (n1.south west) -- (n2.south east) node[midway, yshift = -1.25em] {\pyinline{dataset_train}};
								\draw[decorate, decoration = {mirror}] (n3.south west) -- (n5.south east) node[midway, yshift = -1.25em] {\pyinline{dataset_test}};
							\end{scope}

							\draw (i1) -- (n1.north);
							\draw (i2) -- (n3.north);
						\end{tikzpicture}
						\caption{Логіка розділення списку~\pyinline{dataset_shuffled}}
						\label{fig:dataset-shuffled-slicing}
					\end{figure}

					Після виконання розподілу функція повертає кортеж зі~змінних~\pyinline{dataset_train} та~\pyinline{dataset_test} і~завершує роботу. На цьому підготовка вхідних даних завершена.

				\subsubsection{Класифікація}

					Після розбиття набору даних на~тренувальний та~тестовий, виконується інструкція~\pyinline{summaries = summarize_by_class(dataset_test)}, яка~оцінює статистичні параметри значень ознак для~кожного значення класової змінної за допомогою функції~\pyinline{summarize_by_class()}~(лістинг~\ref{lst:nbc-py-summarize-by-class}).

				\begin{listingpython}{Модуль~\filename{nbc.py}: функція~\pyinline{summarize_by_class()} для~оцінки статистичних параметрів можливих значень кожної ознаки для~усіх класів, до~яких може належати спостереження}{lst:nbc-py-summarize-by-class}
def summarize_by_class(dataset):
    separated = separate_by_class(dataset)
    summaries = {}
    for class_value, data in separated.items():
        summaries[class_value] = summarize(data)

    return summaries
					\end{listingpython}

				\subsubsection{Виведення результату}

	\newpage
	\addsec{Висновки}

	\newpage
	\printbibliography

\end{document}
